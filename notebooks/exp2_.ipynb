{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3011aad-ae9b-4529-91a2-fb2bb98c6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import dagshub\n",
    "import os\n",
    "import mlflow.sklearn\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9004a6b8-efe2-4350-a685-c828883b3512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/external/emotion_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89d38e0c-300c-4aca-a359-aef606251f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"tweet_id\"], inplace=True)\n",
    "df = df[df[\"sentiment\"].isin([\"happiness\", \"sadness\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab8394de-bebe-471a-9498-07b155e3299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:], df.iloc[:, 0], test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "152bbf2b-2b89-4359-9f48-7c4bb3b2f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = LabelEncoder()\n",
    "y_train = encode.fit_transform(y_train)\n",
    "y_test = encode.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a53b272a-dd68-41c2-a3c2-c0029d0bb412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:32: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:32: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\RR\\AppData\\Local\\Temp\\ipykernel_15716\\3701265362.py:32: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  text = re.sub(\"[%s]\" % re.escape(\"\"\"!\"#$%&'()*+,.-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
      "C:\\Users\\RR\\AppData\\Local\\Temp\\ipykernel_15716\\3701265362.py:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text = re.sub('\\s+', ' ', text)\n"
     ]
    }
   ],
   "source": [
    "def lemmatization(text: str) -> str:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    text = text.split()\n",
    "    text=[lemmatizer.lemmatize(y) for y in text]\n",
    "\n",
    "    return \" \".join(text)\n",
    "\n",
    "def remove_stop_words(text: str) -> str:\n",
    "    try:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "    except Exception:\n",
    "        print(\"An error has occurred. If stopwords aren't there please download.\")\n",
    "        raise\n",
    "    else:\n",
    "        text=[i for i in str(text).split() if i not in stop_words]\n",
    "        return \" \".join(text)\n",
    "\n",
    "def removing_numbers(text: str) -> str:\n",
    "    text = \"\".join([i for i in text if not i.isdigit()])\n",
    "    return text\n",
    "\n",
    "def lower_case(text: str) -> str:\n",
    "    text = text.split()\n",
    "\n",
    "    text=[y.lower() for y in text]\n",
    "\n",
    "    return \" \".join(text)\n",
    "\n",
    "def removing_punctuations(text: str) -> str:\n",
    "    ## Remove Punctuations\n",
    "    text = re.sub(\"[%s]\" % re.escape(\"\"\"!\"#$%&'()*+,.-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
    "    text = text.replace(':', \"\")\n",
    "\n",
    "    ## remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "def removing_urls(text: str) -> str:\n",
    "    url_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url_pattern.sub(r\"\", text)\n",
    "\n",
    "def remove_small_sentences(df: pd.DataFrame) -> None:\n",
    "    for i in range(len(df)):\n",
    "        if len(df.text.iloc[i].split()) < 3:\n",
    "            df.text.iloc[i] = np.nan\n",
    "\n",
    "def normalize_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.content = df.content.apply(lambda content : lower_case(content))\n",
    "    df.content = df.content.apply(lambda content : remove_stop_words(content))\n",
    "    df.content = df.content.apply(lambda content : removing_numbers(content))\n",
    "    df.content = df.content.apply(lambda content : removing_punctuations(content))\n",
    "    df.content = df.content.apply(lambda content : removing_urls(content))\n",
    "    df.content = df.content.apply(lambda content : lemmatization(content))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1b4d8b8-f784-4698-8b98-393733f68146",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize_text(X_train)\n",
    "X_test = normalize_text(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd0140f4-e915-4afc-8d78-8da932d272db",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = [CountVectorizer, TfidfVectorizer]\n",
    "models = [LogisticRegression, GradientBoostingClassifier, RandomForestClassifier, XGBClassifier]\n",
    "max_cols = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50a3ee77-6133-4dcb-b30e-856b06007714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23531</th>\n",
       "      <td>quot my problem miss you cause don t quot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8051</th>\n",
       "      <td>that s it done already one proof there s nothi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11499</th>\n",
       "      <td>hungry food steal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31288</th>\n",
       "      <td>foot hurt finally bed will forget crunch over ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18561</th>\n",
       "      <td>really ill atm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21697</th>\n",
       "      <td>chocolatesuze yes yes should especially wine m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19445</th>\n",
       "      <td>kickzfadayz boy better get tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20216</th>\n",
       "      <td>tafe actually quite good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>minute boarding hour home window seat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27810</th>\n",
       "      <td>intel gfx driver situation much better recent ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8299 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content\n",
       "23531          quot my problem miss you cause don t quot\n",
       "8051   that s it done already one proof there s nothi...\n",
       "11499                                  hungry food steal\n",
       "31288  foot hurt finally bed will forget crunch over ...\n",
       "18561                                     really ill atm\n",
       "...                                                  ...\n",
       "21697  chocolatesuze yes yes should especially wine m...\n",
       "19445                 kickzfadayz boy better get tonight\n",
       "20216                           tafe actually quite good\n",
       "3258               minute boarding hour home window seat\n",
       "27810  intel gfx driver situation much better recent ...\n",
       "\n",
       "[8299 rows x 1 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7be04862-0191-468f-853b-befb9b415c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"PriyanshuMewal/mini-project\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"PriyanshuMewal/mini-project\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository PriyanshuMewal/mini-project initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository PriyanshuMewal/mini-project initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer_and_LogisticRegression\n",
      "üèÉ View run CountVectorizer_and_LogisticRegression at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3/runs/b6d4049bebec4812ab019245fa47d072\n",
      "üß™ View experiment at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3\n",
      "CountVectorizer_and_GradientBoostingClassifier\n",
      "üèÉ View run CountVectorizer_and_GradientBoostingClassifier at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3/runs/98db260d4e0c4481a442dc38a09d2b3f\n",
      "üß™ View experiment at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3\n",
      "CountVectorizer_and_RandomForestClassifier\n",
      "üèÉ View run CountVectorizer_and_RandomForestClassifier at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3/runs/9d35980decbb475bb7cf683882d36b7b\n",
      "üß™ View experiment at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3\n",
      "CountVectorizer_and_XGBClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RR\\Desktop\\Python_Projects2\\MLOps Revisied\\Mini Project\\mini_project\\myenv\\Lib\\site-packages\\xgboost\\data.py:399: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n",
      "C:\\Users\\RR\\Desktop\\Python_Projects2\\MLOps Revisied\\Mini Project\\mini_project\\myenv\\Lib\\site-packages\\xgboost\\data.py:399: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run CountVectorizer_and_XGBClassifier at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3/runs/f2c4786ac191461b866c8cf3b3f89a57\n",
      "üß™ View experiment at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3\n",
      "TfidfVectorizer_and_LogisticRegression\n",
      "üèÉ View run TfidfVectorizer_and_LogisticRegression at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3/runs/24d30e21347d49229555c273f5154e0f\n",
      "üß™ View experiment at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3\n",
      "TfidfVectorizer_and_GradientBoostingClassifier\n",
      "üèÉ View run TfidfVectorizer_and_GradientBoostingClassifier at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3/runs/fe337bfb803e4f1992db38d94a29836f\n",
      "üß™ View experiment at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3\n",
      "TfidfVectorizer_and_RandomForestClassifier\n",
      "üèÉ View run TfidfVectorizer_and_RandomForestClassifier at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3/runs/7bce26b371e849c78dc2dfeae493ce70\n",
      "üß™ View experiment at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3\n",
      "TfidfVectorizer_and_XGBClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RR\\Desktop\\Python_Projects2\\MLOps Revisied\\Mini Project\\mini_project\\myenv\\Lib\\site-packages\\xgboost\\data.py:399: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n",
      "C:\\Users\\RR\\Desktop\\Python_Projects2\\MLOps Revisied\\Mini Project\\mini_project\\myenv\\Lib\\site-packages\\xgboost\\data.py:399: UserWarning: Sparse arrays from pandas are converted into dense.\n",
      "  warnings.warn(\"Sparse arrays from pandas are converted into dense.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run TfidfVectorizer_and_XGBClassifier at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3/runs/8c849666f91542039864bebe4dea249f\n",
      "üß™ View experiment at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3\n",
      "üèÉ View run best model at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3/runs/b2981fadb37d45ef9797befa2dd9d686\n",
      "üß™ View experiment at: https://dagshub.com/PriyanshuMewal/mini-project.mlflow/#/experiments/3\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"https://dagshub.com/PriyanshuMewal/mini-project.mlflow\")\n",
    "\n",
    "dagshub.init(repo_owner='PriyanshuMewal', repo_name='mini-project', mlflow=True)\n",
    "\n",
    "mlflow.set_experiment(\"Exp2: Best Combination of fe and model.\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"best model\") as parent:\n",
    "\n",
    "    for vectorizer in vectorizers:\n",
    "        for algo in models:\n",
    "            \n",
    "            with mlflow.start_run(run_name=f\"{vectorizer.__name__}_and_{algo.__name__}\", nested=True) as children:\n",
    "\n",
    "                print(f\"{vectorizer.__name__}_and_{algo.__name__}\")\n",
    "                \n",
    "                vector = vectorizer(max_features=max_cols)\n",
    "                X_train_trf_mat = vector.fit_transform(X_train[\"content\"].values)\n",
    "                X_test_trf_mat = vector.transform(X_test[\"content\"].values)\n",
    "            \n",
    "                X_train_trf = pd.DataFrame.sparse.from_spmatrix(X_train_trf_mat, columns=vector.get_feature_names_out())\n",
    "                X_test_trf = pd.DataFrame.sparse.from_spmatrix(X_test_trf_mat, columns=vector.get_feature_names_out())\n",
    "\n",
    "                model = algo()\n",
    "                model.fit(X_train_trf, y_train)\n",
    "            \n",
    "                y_pred = model.predict(X_test_trf)\n",
    "            \n",
    "                accuracy = accuracy_score(y_pred, y_test)\n",
    "                precision = precision_score(y_pred, y_test)\n",
    "                recall = recall_score(y_pred, y_test)\n",
    "                f1 = f1_score(y_pred, y_test)\n",
    "                \n",
    "                # log params\n",
    "                mlflow.log_param(\"max_features\", max_cols)\n",
    "                mlflow.log_param(\"test_size\", test_size)\n",
    "                mlflow.log_param(\"vectorizer\", f\"{vectorizer.__name__}\")\n",
    "                mlflow.log_param(\"model\", f\"{algo.__name__}\")\n",
    "                \n",
    "                # log metrics\n",
    "                mlflow.log_metric(\"accuracy\", accuracy)\n",
    "                mlflow.log_metric('precision', precision)\n",
    "                mlflow.log_metric('recall', recall)\n",
    "                mlflow.log_metric('f1_score', f1)\n",
    "                \n",
    "                # log source\n",
    "                # notebook_path = \"data_ingestion.ipynb\"\n",
    "                # os.system(f\"jupyter nbconvert --to notebook --execute --inplace {notebook_path}\")\n",
    "                # mlflow.log_artifact(notebook_path)\n",
    "                \n",
    "                # log model\n",
    "                mlflow.sklearn.log_model(model, name=f\"{algo.__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdabaf29-c0ab-465e-a1b1-dd1b478e055e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
