{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3011aad-ae9b-4529-91a2-fb2bb98c6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import dagshub\n",
    "import os\n",
    "import mlflow.sklearn\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9004a6b8-efe2-4350-a685-c828883b3512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/external/emotion_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89d38e0c-300c-4aca-a359-aef606251f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"tweet_id\"], inplace=True)\n",
    "df = df[df[\"sentiment\"].isin([\"happiness\", \"sadness\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab8394de-bebe-471a-9498-07b155e3299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:], df.iloc[:, 0], test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "152bbf2b-2b89-4359-9f48-7c4bb3b2f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = LabelEncoder()\n",
    "y_train = encode.fit_transform(y_train)\n",
    "y_test = encode.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53b272a-dd68-41c2-a3c2-c0029d0bb412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:32: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:32: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\RR\\AppData\\Local\\Temp\\ipykernel_2444\\3701265362.py:32: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  text = re.sub(\"[%s]\" % re.escape(\"\"\"!\"#$%&'()*+,.-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
      "C:\\Users\\RR\\AppData\\Local\\Temp\\ipykernel_2444\\3701265362.py:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text = re.sub('\\s+', ' ', text)\n"
     ]
    }
   ],
   "source": [
    "def lemmatization(text: str) -> str:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    text = text.split()\n",
    "    text=[lemmatizer.lemmatize(y) for y in text]\n",
    "\n",
    "    return \" \".join(text)\n",
    "\n",
    "def remove_stop_words(text: str) -> str:\n",
    "    try:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "    except Exception:\n",
    "        print(\"An error has occurred. If stopwords aren't there please download.\")\n",
    "        raise\n",
    "    else:\n",
    "        text=[i for i in str(text).split() if i not in stop_words]\n",
    "        return \" \".join(text)\n",
    "\n",
    "def removing_numbers(text: str) -> str:\n",
    "    text = \"\".join([i for i in text if not i.isdigit()])\n",
    "    return text\n",
    "\n",
    "def lower_case(text: str) -> str:\n",
    "    text = text.split()\n",
    "\n",
    "    text=[y.lower() for y in text]\n",
    "\n",
    "    return \" \".join(text)\n",
    "\n",
    "def removing_punctuations(text: str) -> str:\n",
    "    ## Remove Punctuations\n",
    "    text = re.sub(\"[%s]\" % re.escape(\"\"\"!\"#$%&'()*+,.-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
    "    text = text.replace(':', \"\")\n",
    "\n",
    "    ## remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "def removing_urls(text: str) -> str:\n",
    "    url_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url_pattern.sub(r\"\", text)\n",
    "\n",
    "def remove_small_sentences(df: pd.DataFrame) -> None:\n",
    "    for i in range(len(df)):\n",
    "        if len(df.text.iloc[i].split()) < 3:\n",
    "            df.text.iloc[i] = np.nan\n",
    "\n",
    "def normalize_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.content = df.content.apply(lambda content : lower_case(content))\n",
    "    df.content = df.content.apply(lambda content : remove_stop_words(content))\n",
    "    df.content = df.content.apply(lambda content : removing_numbers(content))\n",
    "    df.content = df.content.apply(lambda content : removing_punctuations(content))\n",
    "    df.content = df.content.apply(lambda content : removing_urls(content))\n",
    "    df.content = df.content.apply(lambda content : lemmatization(content))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b4d8b8-f784-4698-8b98-393733f68146",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize_text(X_train)\n",
    "X_test = normalize_text(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0140f4-e915-4afc-8d78-8da932d272db",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = [CountVectorizer, TfidfVectorizer]\n",
    "models = [LogisticRegression, GradientBoostingClassifier, RandomForestClassifier, XGBClassifier]\n",
    "max_cols = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a3ee77-6133-4dcb-b30e-856b06007714",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be04862-0191-468f-853b-befb9b415c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"https://dagshub.com/PriyanshuMewal/mini-project.mlflow\")\n",
    "\n",
    "dagshub.init(repo_owner='PriyanshuMewal', repo_name='mini-project', mlflow=True)\n",
    "\n",
    "mlflow.set_experiment(\"Exp2: Best Combination of fe and model.\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"best model\") as parent:\n",
    "\n",
    "    for vectorizer in vectorizers:\n",
    "        for algo in models:\n",
    "            \n",
    "            with mlflow.start_run(run_name=f\"{vectorizer.__name__}_and_{algo.__name__}\", nested=True) as children:\n",
    "\n",
    "                print(f\"{vectorizer.__name__}_and_{algo.__name__}\")\n",
    "                \n",
    "                vector = vectorizer(max_features=max_cols)\n",
    "                X_train_trf_mat = vector.fit_transform(X_train[\"content\"].values)\n",
    "                X_test_trf_mat = vector.transform(X_test[\"content\"].values)\n",
    "            \n",
    "                X_train_trf = pd.DataFrame.sparse.from_spmatrix(X_train_trf_mat, columns=vector.get_feature_names_out())\n",
    "                X_test_trf = pd.DataFrame.sparse.from_spmatrix(X_test_trf_mat, columns=vector.get_feature_names_out())\n",
    "\n",
    "                model = algo()\n",
    "                model.fit(X_train_trf, y_train)\n",
    "            \n",
    "                y_pred = model.predict(X_test_trf)\n",
    "            \n",
    "                accuracy = accuracy_score(y_pred, y_test)\n",
    "                precision = precision_score(y_pred, y_test)\n",
    "                recall = recall_score(y_pred, y_test)\n",
    "                f1 = f1_score(y_pred, y_test)\n",
    "                \n",
    "                # log params\n",
    "                mlflow.log_param(\"max_features\", max_cols)\n",
    "                mlflow.log_param(\"test_size\", test_size)\n",
    "                mlflow.log_param(\"vectorizer\", f\"{vectorizer.__name__}\")\n",
    "                mlflow.log_param(\"model\", f\"{algo.__name__}\")\n",
    "                \n",
    "                # log metrics\n",
    "                mlflow.log_metric(\"accuracy\", accuracy)\n",
    "                mlflow.log_metric('precision', precision)\n",
    "                mlflow.log_metric('recall', recall)\n",
    "                mlflow.log_metric('f1_score', f1)\n",
    "                \n",
    "                # log source\n",
    "                # notebook_path = \"data_ingestion.ipynb\"\n",
    "                # os.system(f\"jupyter nbconvert --to notebook --execute --inplace {notebook_path}\")\n",
    "                # mlflow.log_artifact(notebook_path)\n",
    "                \n",
    "                # log model\n",
    "                mlflow.sklearn.log_model(model, name=f\"{algo.__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdabaf29-c0ab-465e-a1b1-dd1b478e055e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
